{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook has examples on how to simulate chats and how to run bulk evaluation.\n",
    "\n",
    " â— Before running this notebook, make sure you install the dependencies defined in `src/requirements-eval.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "The evaluation module depends on other modules from project Acumen, specifically for loading conversation histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SOURCE_DIR = \"../../src\"\n",
    "sys.path.append(SOURCE_DIR)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(SOURCE_DIR, \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app import create_app_context\n",
    "from config import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "app_ctx = create_app_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_QUERIES_CSV_PATH = \"./evaluation_sample_initial_queries.csv\"\n",
    "SIMULATION_OUTPUT_PATH = \"./simulated_chats/patient_4\"\n",
    "EVALUATION_RESULTS_PATH = os.path.join(SIMULATION_OUTPUT_PATH, \"evaluation_results\")\n",
    "\n",
    "PATIENT_TIMELINE_REFERENCE_PATH = \"./reference/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate chats\n",
    "\n",
    "Below, we simulate conversations based on queries loaded from a `csv`. This csv must have one column with the **patient ID** (as expected by the agents) and an **initial query column**, that will serve as the conversation starter. Optionally, we may include an additional column for **follow-up questions**.\n",
    "\n",
    "Each row in the `csv` contains a single follow-up question. When `group_followups=True` (default), the system will combine all follow-ups with the same patient ID and initial query into a single conversation flow, asking them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.chat_simulator import ProceedUser, LLMUser, ChatSimulator\n",
    "\n",
    "initial_query = \"Orchestrator: Prepare tumor board for Patient ID: patient_4\"\n",
    "\n",
    "# user = ProceedUser()\n",
    "user = LLMUser()\n",
    "\n",
    "chat_simulator = ChatSimulator(\n",
    "    simulated_user=user,\n",
    "    group_chat_kwargs={\n",
    "        \"app_ctx\": app_ctx,\n",
    "    },\n",
    "    trial_count=1,\n",
    "    max_turns=10,\n",
    "    output_folder_path=SIMULATION_OUTPUT_PATH,\n",
    "    save_readable_history=True,\n",
    "    print_messages=False,\n",
    "    raise_errors=True,\n",
    ")\n",
    "\n",
    "chat_simulator.load_initial_queries(\n",
    "    csv_file_path=INITIAL_QUERIES_CSV_PATH,\n",
    "    patients_id_column=\"Patient ID\",\n",
    "    initial_queries_column=\"Initial Query\",\n",
    "    followup_column=\"Possible Follow up\",\n",
    "    group_followups=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling `load_initial_queries` to load the data needed for simulating chats, you may pass them directly to the class constructor:\n",
    "\n",
    "```python\n",
    "patient_id = \"patient_4\"\n",
    "initial_query = \"Orchestrator: Prepare tumor board for Patient ID: patient_4\"\n",
    "# At least an empty string must be given as a followup question\n",
    "followup_questions = [\"\"]\n",
    "\n",
    "user = LLMUser()\n",
    "\n",
    "chat_simulator = ChatSimulator(\n",
    "    simulated_user=user,\n",
    "    group_chat_kwargs={\n",
    "        \"all_agents_config\": agent_config,\n",
    "        \"data_access\": data_access,\n",
    "    },\n",
    "    patients_id=[patient_id],\n",
    "    initial_queries=[initial_query],\n",
    "    followup_questions=[followup_questions],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of output is generated when simulating chats, which might make this file too big for opening.\n",
    "\n",
    "For that reason, please clear the output of at least the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat_simulator.simulate_chats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ad-hoc cases, you may also call `chat_simulator.chat` directly:\n",
    "\n",
    "```python\n",
    "chat_simulator.chat(\n",
    "    patients_id=[patient_id],\n",
    "    initial_queries=[initial_query],\n",
    "    followup_questions=[followup_questions],\n",
    "    max_turns=5\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Input data\n",
    "Below, we evaluate conversations simulated in the previous step. For evaluation, we need the serialized chat context, which is out case it the `json` file generated by the simulation.\n",
    "\n",
    "The deployed application also stores conversations whenever they are cleared with the message `@Orchestrator clear`. Naturally, that data can also be used for evaluation.\n",
    "\n",
    "### Reference based metrics\n",
    "Below you will also notice that some metric (such as `RougeMetric` and `TBFactMetric`) require ground truth data to generate scores. Internally, these metrics, will load `.txt` files from a provided folder. **The `txt` file name must be the respective `patient_id`**\n",
    "\n",
    "> ðŸ’¡**Tip**: The chat context `json` includes a top-level key `patient_id` that is used to track what patient was the target of the conversation, and thus used to match the correct reference data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion\n",
    "\n",
    "from evaluation.evaluator import Evaluator\n",
    "from evaluation.metrics.agent_selection import AgentSelectionEvaluator\n",
    "from evaluation.metrics.context_relevancy import ContextRelevancyEvaluator\n",
    "from evaluation.metrics.info_aggregation import InformationAggregationEvaluator\n",
    "from evaluation.metrics.rouge import RougeMetric\n",
    "from evaluation.metrics.intent_resolution import IntentResolutionEvaluator\n",
    "from evaluation.metrics.factuality import TBFactMetric\n",
    "from evaluation.metrics.turn_by_turn_agent_selection import TurnByTurnAgentSelectionEvaluator\n",
    "from evaluation.metrics.turn_by_turn_with_history import TurnByTurnEvaluatorWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_service = AzureChatCompletion(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    ")\n",
    "\n",
    "#evaluate orchestrator's agent selection by passing full chat history to LLM judge\n",
    "agent_selection_evaluator = AgentSelectionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "#evaluate orchestrator's agent selection by passing only individual turn to LLM judge\n",
    "turn_by_turn_evaluator = TurnByTurnAgentSelectionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    #scenario=\"my_scenario\", #could be used to differentiate scenarios, leave empty to use default folder name\n",
    "    #agent_name=\"agent_name\", #specify orchestrator agent name, leave empty to use default agent name from config\n",
    ")\n",
    "\n",
    "intent_resolution_evaluator = IntentResolutionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "information_aggregation_evaluator = InformationAggregationEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    ")\n",
    "\n",
    "context_relevancy_evaluator = ContextRelevancyEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    agent_name=\"PatientHistory\",\n",
    "    context_window=5,\n",
    ")\n",
    "\n",
    "rouge_metric = RougeMetric(\n",
    "    agent_name=\"PatientHistory\",\n",
    "    reference_dir_path=PATIENT_TIMELINE_REFERENCE_PATH\n",
    ")\n",
    "\n",
    "tbfact_metric = TBFactMetric(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    agent_name=\"PatientHistory\",\n",
    "    reference_dir_path=PATIENT_TIMELINE_REFERENCE_PATH,\n",
    "    context_window=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_system_prompt = \"\"\"\n",
    "You are an expert evaluator of medical AI assistants. Your task is to evaluate whether the AI orchestrator (called \"Orchestrator\") \n",
    "correctly understood and addressed the user's intent in each turn of the conversation.\n",
    "\n",
    "Rate how well the orchestrator understood and addressed the user's intent on a scale from 1 to 5:\n",
    "1: Poor - Completely misunderstood or failed to address user's intent\n",
    "2: Below Average - Partially misunderstood or inadequately addressed intent\n",
    "3: Average - Basic understanding but could have addressed intent better\n",
    "4: Good - Clear understanding and appropriate response to intent\n",
    "5: Excellent - Perfect understanding and optimal response to user's intent\n",
    "\n",
    "Your response must begin with \"Rating: X\" where X is your score (1-5), followed by your detailed explanation.\n",
    "\"\"\"\n",
    "#evaluate the agent-user conversation by passing the full conversation to llm judge\n",
    "turn_by_turn_intent_evaluator = TurnByTurnAgentSelectionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    system_prompt=intent_system_prompt,\n",
    "    metric_name=\"turn_by_turn_intent_resolution\",\n",
    "    description=\"Evaluates intent understanding and resolution for each turn\"\n",
    ")\n",
    "\n",
    "#evaluate the agent-user conversation by turn, considering the context of previous turns\n",
    "turn_by_turn_intent_evaluator_with_context = TurnByTurnEvaluatorWithContext(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    system_prompt=intent_system_prompt,\n",
    "    metric_name=\"turn_by_turn_intent_resolution\",\n",
    "    agent_name=\"Orchestrator\", #could pass any other defined agents\n",
    "    description=\"Evaluates intent understanding and resolution for each turn\"\n",
    ")\n",
    "\n",
    "info_agg_system_prompt = \"\"\"\n",
    "You are an expert evaluator of medical AI assistants. Your task is to evaluate a conversation between a user and an AI orchestrator (called \"Orchestrator\") that coordinates multiple specialized medical agents.\n",
    "\n",
    "Focus specifically on the orchestrator's ability to INTEGRATE INFORMATION FROM MULTIPLE AGENTS to form comprehensive answers. Consider:\n",
    "1. Did the orchestrator effectively combine information from different specialized agents?\n",
    "2. Did it synthesize potentially contradicting information appropriately?\n",
    "3. Did it create coherent, comprehensive answers that draw on multiple knowledge sources?\n",
    "4. Did it identify connections between information from different agents?\n",
    "\n",
    "Rate the orchestrator's information integration ability on a scale from 1 to 5:\n",
    "1: Poor - Failed to integrate information; simply repeated individual agent outputs or used only single sources\n",
    "2: Below Average - Minimal integration; mostly relied on individual agents with little synthesis\n",
    "3: Average - Basic integration of information; combined some facts but missed opportunities for deeper synthesis\n",
    "4: Good - Strong integration; effectively combined information from multiple agents into coherent responses\n",
    "5: Excellent - Superior integration; seamlessly synthesized information from multiple agents, creating insights beyond what any single agent provided\n",
    "\n",
    "Your response must begin with \"Rating: X\" where X is your score (1-5), followed by your detailed explanation.\n",
    "\n",
    "IMPORTANT: Some conversations may end abruptly due to turn limits. In these cases, evaluate based on what was accomplished up to that point.as_integer_ratio\n",
    "\"\"\"\n",
    "\n",
    "#evaluate the agent-user conversation by passing the full conversation to llm judge\n",
    "turn_by_turn_info_evaluator = TurnByTurnAgentSelectionEvaluator(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    system_prompt=info_agg_system_prompt,\n",
    "    metric_name=\"turn_by_turn_information_aggregation\",\n",
    "    description=\"Evaluates information aggregation for each turn\"\n",
    ")\n",
    "\n",
    "#evaluate the agent-user conversation by turn, considering the context of previous turns\n",
    "turn_by_turn_info_evaluator_with_context = TurnByTurnEvaluatorWithContext(\n",
    "    evaluation_llm_service=llm_service,\n",
    "    system_prompt=info_agg_system_prompt,\n",
    "    metric_name=\"turn_by_turn_information_aggregation\",\n",
    "    agent_name=\"Orchestrator\", #could pass any other defined agents\n",
    "    description=\"Evaluates information aggregation for each turn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    metrics=[\n",
    "        agent_selection_evaluator,\n",
    "        # intent_resolution_evaluator,\n",
    "        # information_aggregation_evaluator,\n",
    "        # context_relevancy_evaluator,\n",
    "        # rouge_metric,\n",
    "        tbfact_metric,\n",
    "        #turn_by_turn_intent_evaluator_with_context,\n",
    "        #turn_by_turn_info_evaluator_with_context,\n",
    "        #turn_by_turn_evaluator,\n",
    "    ],\n",
    "    output_folder_path=EVALUATION_RESULTS_PATH,\n",
    ")\n",
    "\n",
    "evaluator.load_chat_contexts(SIMULATION_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `ChatSimulator` class, you may skip `load_chat_contexts` by passing it directly in the constructor:\n",
    "\n",
    "```python\n",
    "from data_models.chat_context import ChatContext\n",
    "\n",
    "chats_contexts: list[ChatContext]\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    chats_contexts=chats_contexts\n",
    "    metrics=[\n",
    "        ...\n",
    "    ],\n",
    "    output_folder_path=SIMULATION_OUTPUT_PATH,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = await evaluator.evaluate()\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick show of results, we print results below, but for better understanding the scores and behaviour of agents, ideally you drill down the dictionary generated in the previous step (also saved as a `json` in the evaluation output folder). It includes all individual results, explanations and details specific of each metric.\n",
    "\n",
    ">ðŸ’¡**Note:** When no reference data is provided, that instance will result in `Error: No reference found for patient ID: `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_name, metric_result in evaluation_results[\"metrics\"].items():\n",
    "    print(f\"{metric_name}: average_score: {metric_result[\"average_score\"]} | num_errors: {metric_result[\"num_errors\"]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
